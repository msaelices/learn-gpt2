{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Token & Position Embeddings - Interactive Notebook\n",
    "\n",
    "This notebook helps you explore and visualize embeddings interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# When finished, uncomment the following sentence and import your own solution\n",
    "from solution import Embeddings\n",
    "# from problem import Embeddings\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings with small dimensions for visualization\n",
    "embeddings = Embeddings(\n",
    "    vocab_size=1000,  # Smaller vocab for demo\n",
    "    n_positions=100,  # Shorter sequences\n",
    "    n_embd=64,        # Smaller embedding dimension\n",
    "    embd_pdrop=0.1\n",
    ")\n",
    "\n",
    "print(embeddings)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in embeddings.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Input tokens:\\n{input_ids}\")\n",
    "\n",
    "# Set to eval mode to disable dropout for visualization\n",
    "embeddings.eval()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = embeddings(input_ids)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Expected shape: ({batch_size}, {seq_len}, 64)\")\n",
    "print(f\"\\nFirst few values of output[0, 0, :10]:\\n{output[0, 0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings for a few tokens\n",
    "token_ids = [0, 1, 2, 50, 100, 500]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, token_id in enumerate(token_ids):\n",
    "    # Get embedding for this token\n",
    "    token_tensor = torch.tensor([[token_id]])\n",
    "    with torch.no_grad():\n",
    "        emb = embeddings.wte(token_tensor).squeeze()\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].bar(range(len(emb)), emb.numpy())\n",
    "    axes[idx].set_title(f\"Token ID: {token_id}\")\n",
    "    axes[idx].set_xlabel(\"Embedding Dimension\")\n",
    "    axes[idx].set_ylabel(\"Value\")\n",
    "    axes[idx].set_ylim([-2, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Token Embeddings for Different Tokens\", y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Each token has a unique embedding vector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get position embeddings for first 20 positions\n",
    "n_positions_viz = 20\n",
    "position_ids = torch.arange(0, n_positions_viz).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_emb = embeddings.wpe(position_ids).squeeze(0).numpy()\n",
    "\n",
    "# Plot as heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos_emb.T, aspect='auto', cmap='RdBu_r', interpolation='nearest')\n",
    "plt.colorbar(label='Embedding Value')\n",
    "plt.xlabel('Position in Sequence')\n",
    "plt.ylabel('Embedding Dimension')\n",
    "plt.title('Position Embeddings Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each position has its own learned embedding pattern!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Token vs Position vs Combined Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple input\n",
    "test_input = torch.tensor([[42, 100, 256]])\n",
    "position_ids = torch.arange(0, 3).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    token_emb = embeddings.wte(test_input)\n",
    "    pos_emb = embeddings.wpe(position_ids)\n",
    "    combined = token_emb + pos_emb\n",
    "\n",
    "# Plot all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Token embeddings\n",
    "im1 = axes[0].imshow(token_emb.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "axes[0].set_title('Token Embeddings')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Position embeddings\n",
    "im2 = axes[1].imshow(pos_emb.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "axes[1].set_title('Position Embeddings')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Combined embeddings\n",
    "im3 = axes[2].imshow(combined.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "axes[2].set_title('Combined Embeddings (Token + Position)')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Combined = Token + Position (element-wise addition)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dropout Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings with high dropout for visualization\n",
    "emb_with_dropout = Embeddings(\n",
    "    vocab_size=1000,\n",
    "    n_positions=100,\n",
    "    n_embd=64,\n",
    "    embd_pdrop=0.5  # 50% dropout!\n",
    ")\n",
    "\n",
    "test_input = torch.tensor([[1, 2, 3, 4, 5]])\n",
    "\n",
    "# Training mode (dropout active)\n",
    "emb_with_dropout.train()\n",
    "outputs_train = []\n",
    "for i in range(3):\n",
    "    out = emb_with_dropout(test_input)\n",
    "    outputs_train.append(out.detach())\n",
    "\n",
    "# Eval mode (no dropout)\n",
    "emb_with_dropout.eval()\n",
    "with torch.no_grad():\n",
    "    output_eval = emb_with_dropout(test_input)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i in range(3):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    im = ax.imshow(outputs_train[i].squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "    ax.set_title(f'Training Mode - Run {i+1} (with dropout)')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Embedding Dimension')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "im = axes[1, 1].imshow(output_eval.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[1, 1].set_title('Eval Mode (no dropout)')\n",
    "axes[1, 1].set_xlabel('Position')\n",
    "axes[1, 1].set_ylabel('Embedding Dimension')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Training runs show different patterns (dropout), but eval is consistent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Different Embedding Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different embedding dimensions\n",
    "emb_dims = [32, 64, 128, 256]\n",
    "test_input = torch.tensor([[42]])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, n_embd in enumerate(emb_dims):\n",
    "    emb = Embeddings(vocab_size=1000, n_positions=100, n_embd=n_embd)\n",
    "    emb.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = emb(test_input).squeeze()\n",
    "    \n",
    "    axes[idx].bar(range(len(out)), out.numpy())\n",
    "    axes[idx].set_title(f'Embedding Dimension: {n_embd}')\n",
    "    axes[idx].set_xlabel('Dimension Index')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].set_ylim([-2, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Same Token with Different Embedding Dimensions', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Larger embedding dimensions provide more capacity to represent information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation\n",
    "\n",
    "Run the tests to make sure everything works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "embeddings = Embeddings()\n",
    "input_ids = torch.randint(0, 50257, (2, 10))\n",
    "output = embeddings(input_ids)\n",
    "\n",
    "print(\"âœ“ Module initializes correctly\")\n",
    "print(f\"âœ“ Forward pass works: {input_ids.shape} â†’ {output.shape}\")\n",
    "print(f\"âœ“ Expected shape: (2, 10, 768)\")\n",
    "print(f\"âœ“ Actual shape: {output.shape}\")\n",
    "print(\"\\nâœ… Basic functionality verified!\")\n",
    "print(\"\\nRun 'python -m pytest test_embeddings.py -v' for complete validation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
