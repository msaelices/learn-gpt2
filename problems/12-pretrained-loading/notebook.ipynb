{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12: Loading Pretrained Weights - Interactive Notebook\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** This is the final problem in the GPT-2 learning path.\n",
    "\n",
    "Load pretrained GPT-2 weights and use the model for real text generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../09-gpt2-config')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from solution import GPT2Model, GPT2Config\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained GPT-2 small\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "config = model.config\n",
    "\n",
    "print(f'\\nModel configuration:')\n",
    "print(f'  Layers: {config.n_layer}')\n",
    "print(f'  Embedding dim: {config.n_embd}')\n",
    "print(f'  Attention heads: {config.n_head}')\n",
    "print(f'  Vocabulary size: {config.vocab_size:,}')\n",
    "print(f'  Context length: {config.n_positions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Outputs Match HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load HuggingFace model for comparison\n",
    "hf_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set both to eval mode\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "\n",
    "# Test with a simple prompt\n",
    "text = \"Hello, my name is\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "print(f'Input text: \"{text}\"')\n",
    "print(f'Input tokens: {input_ids[0].tolist()}')\n",
    "\n",
    "# Get outputs from both models\n",
    "with torch.no_grad():\n",
    "    our_logits = model(input_ids)\n",
    "    hf_logits = hf_model(input_ids).logits\n",
    "\n",
    "# Compare\n",
    "max_diff = (our_logits - hf_logits).abs().max().item()\n",
    "mean_diff = (our_logits - hf_logits).abs().mean().item()\n",
    "\n",
    "print(f'\\nOutput comparison:')\n",
    "print(f'  Max difference: {max_diff:.2e}')\n",
    "print(f'  Mean difference: {mean_diff:.2e}')\n",
    "print(f'  Outputs match: {torch.allclose(our_logits, hf_logits, atol=1e-5)}')\n",
    "\n",
    "# Get top predictions\n",
    "our_next_token = our_logits[0, -1].argmax()\n",
    "hf_next_token = hf_logits[0, -1].argmax()\n",
    "\n",
    "print(f'\\nNext token prediction:')\n",
    "print(f'  Our model: {tokenizer.decode(our_next_token)}')\n",
    "print(f'  HuggingFace: {tokenizer.decode(hf_next_token)}')\n",
    "print(f'  Match: {our_next_token == hf_next_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT2Model to use for generation\n",
    "        tokenizer: Tokenizer for encoding/decoding\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate tokens one at a time\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get logits for next token\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop if we hit max context length\n",
    "            if input_ids.shape[1] >= 1024:\n",
    "                break\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Try some generation\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The meaning of life is\",\n",
    "    \"In the year 2050,\",\n",
    "]\n",
    "\n",
    "print('Text Generation Examples:\\n')\n",
    "print('=' * 80)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=30, temperature=0.7)\n",
    "    print(f'\\nPrompt: \"{prompt}\"')\n",
    "    print(f'Generated: {generated}')\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Token Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for a specific prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "\n",
    "# Get probabilities for next token\n",
    "next_token_probs = torch.softmax(logits[0, -1], dim=-1)\n",
    "\n",
    "# Get top-20 predictions\n",
    "top_k = 20\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart of top predictions\n",
    "ax1.barh(range(top_k), top_probs.numpy(), color='steelblue')\n",
    "ax1.set_yticks(range(top_k))\n",
    "ax1.set_yticklabels([f'{tok}' for tok in top_tokens])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Probability')\n",
    "ax1.set_title(f'Top-{top_k} Next Token Predictions\\nAfter: \"{prompt}\"')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Distribution of all probabilities\n",
    "ax2.hist(next_token_probs.numpy(), bins=100, color='coral', alpha=0.7)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_ylabel('Number of Tokens')\n",
    "ax2.set_title('Distribution of All Token Probabilities')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"')\n",
    "print(f'Most likely next token: \"{top_tokens[0]}\" ({top_probs[0].item():.2%} probability)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Weight Distributions: Pretrained vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random model for comparison\n",
    "torch.manual_seed(42)\n",
    "random_model = GPT2Model(config)\n",
    "\n",
    "# Get weights from both models\n",
    "pretrained_weights = model.h[0].attn.c_attn.weight.detach().flatten().numpy()\n",
    "random_weights = random_model.h[0].attn.c_attn.weight.detach().flatten().numpy()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Pretrained weights\n",
    "ax1.hist(pretrained_weights, bins=100, color='green', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(pretrained_weights.mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {pretrained_weights.mean():.4f}')\n",
    "ax1.set_xlabel('Weight Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title(f'Pretrained Weights\\nStd: {pretrained_weights.std():.4f}')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Random weights\n",
    "ax2.hist(random_weights, bins=100, color='gray', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(random_weights.mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {random_weights.mean():.4f}')\n",
    "ax2.set_xlabel('Weight Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title(f'Random Init Weights\\nStd: {random_weights.std():.4f}')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Overlay comparison\n",
    "ax3.hist(pretrained_weights, bins=100, color='green', alpha=0.5, label='Pretrained')\n",
    "ax3.hist(random_weights, bins=100, color='gray', alpha=0.5, label='Random')\n",
    "ax3.set_xlabel('Weight Value')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Overlay Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Pretrained weights have learned structure!')\n",
    "print('Random weights are just noise.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for some interesting tokens\n",
    "words = ['king', 'queen', 'man', 'woman', 'happy', 'sad', 'good', 'bad', 'hot', 'cold']\n",
    "token_ids = [tokenizer.encode(word)[0] for word in words]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = model.wte.weight[token_ids].detach()\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity = torch.mm(embeddings, embeddings.t())\n",
    "# Normalize\n",
    "norms = embeddings.norm(dim=1, keepdim=True)\n",
    "similarity = similarity / (norms @ norms.t())\n",
    "\n",
    "# Plot similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity.numpy(), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.title('Semantic Similarity in Embedding Space')\n",
    "\n",
    "# Add values\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        plt.text(j, i, f'{similarity[i,j]:.2f}',\n",
    "                ha='center', va='center',\n",
    "                color='white' if abs(similarity[i,j]) > 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Semantically related words have higher similarity!')\n",
    "print('E.g., king-queen, man-woman, happy-sad, good-bad, hot-cold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy(model, tokenizer, prompt, max_length=30):\n",
    "    \"\"\"Greedy generation (always pick most likely token).\"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token = logits[0, -1].argmax(dim=-1, keepdim=True)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt, max_length=30, temperature=1.0):\n",
    "    \"\"\"Sampling with temperature.\"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[0, -1] / temperature\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Compare strategies\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "print('Comparing Generation Strategies\\n')\n",
    "print('=' * 80)\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "\n",
    "# Greedy\n",
    "greedy_output = generate_greedy(model, tokenizer, prompt, max_length=20)\n",
    "print(f'Greedy (deterministic):')\n",
    "print(f'{greedy_output}\\n')\n",
    "\n",
    "# Temperature sampling\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    sampled_output = generate_sample(model, tokenizer, prompt, max_length=20, temperature=temp)\n",
    "    print(f'Temperature={temp}:')\n",
    "    print(f'{sampled_output}\\n')\n",
    "\n",
    "print('Lower temperature = more conservative')\n",
    "print('Higher temperature = more creative/random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ðŸŽ‰ CONGRATULATIONS! ðŸŽ‰\\n')\n",
    "print('You\\'ve completed all 12 problems of the GPT-2 learning path!')\n",
    "print('\\nYou now have:')\n",
    "print('  âœ… Deep understanding of transformer architecture')\n",
    "print('  âœ… Complete GPT-2 implementation from scratch')\n",
    "print('  âœ… Ability to load and use pretrained models')\n",
    "print('  âœ… Hands-on experience with every component')\n",
    "print('\\nWhat you\\'ve built:')\n",
    "print(f'  â€¢ Token and position embeddings')\n",
    "print(f'  â€¢ Scaled dot-product attention')\n",
    "print(f'  â€¢ Multi-head attention with causal masking')\n",
    "print(f'  â€¢ Feedforward networks with GELU')\n",
    "print(f'  â€¢ Layer normalization and residuals')\n",
    "print(f'  â€¢ Complete transformer blocks')\n",
    "print(f'  â€¢ Full GPT-2 model assembly')\n",
    "print(f'  â€¢ Proper weight initialization')\n",
    "print(f'  â€¢ Pretrained weight loading')\n",
    "print(f'\\nTotal lines of code: ~1,500+')\n",
    "print(f'Total tests written: 200+')\n",
    "print(f'All tests passing: âœ…')\n",
    "print('\\nNext steps:')\n",
    "print('  â†’ Experiment with text generation techniques')\n",
    "print('  â†’ Implement KV caching for efficient generation')\n",
    "print('  â†’ Try fine-tuning on custom datasets')\n",
    "print('  â†’ Explore other architectures (LLaMA, GPT-3, etc.)')\n",
    "print('  â†’ Build your own LLM applications!')\n",
    "print('\\nYou\\'re now ready to understand and work with modern LLMs! ðŸš€')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
