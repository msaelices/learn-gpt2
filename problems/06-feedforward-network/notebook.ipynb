{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Feedforward Network - Interactive Notebook\n\nExplore GELU activation and feedforward networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When finished, uncomment the following sentence and import your own solution\n",
    "from solution import FeedForward, NewGELU\n",
    "# from problem import FeedForward, NewGELU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize GELU vs ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GELU with ReLU\nx = torch.linspace(-3, 3, 1000)\n\ngelu = NewGELU()\ngelu_output = gelu(x)\nrelu_output = torch.relu(x)\npytorch_gelu = nn.GELU(approximate='tanh')(x)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x.numpy(), gelu_output.detach().numpy(), label='Our GELU', linewidth=2)\nplt.plot(x.numpy(), pytorch_gelu.detach().numpy(), '--', label='PyTorch GELU', linewidth=2, alpha=0.7)\nplt.plot(x.numpy(), relu_output.numpy(), label='ReLU', linewidth=2, alpha=0.7)\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('GELU vs ReLU Activation Functions')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint('Notice:')\nprint('- GELU is smooth (no sharp corner at 0 like ReLU)')\nprint('- GELU can output small negative values')\nprint('- Our GELU matches PyTorch GELU closely')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feedforward network\nn_embd, n_inner = 768, 3072  # GPT-2 small config\nff = FeedForward(n_embd=n_embd, n_inner=n_inner, dropout=0.0)\nff.eval()\n\nprint(f'Input dimension: {n_embd}')\nprint(f'Hidden dimension: {n_inner} (4x expansion)')\nprint(f'Output dimension: {n_embd}')\nprint(f'\\nParameters: {sum(p.numel() for p in ff.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample data\nseq_len = 10\nx = torch.randn(1, seq_len, n_embd)\n\nwith torch.no_grad():\n    output = ff(x)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nim1 = axes[0].imshow(x.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\naxes[0].set_title('Input')\naxes[0].set_xlabel('Position')\naxes[0].set_ylabel('Dimension')\nplt.colorbar(im1, ax=axes[0])\n\nim2 = axes[1].imshow(output.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\naxes[1].set_title('Output (After FFN)')\naxes[1].set_xlabel('Position')\naxes[1].set_ylabel('Dimension')\nplt.colorbar(im2, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\nprint('Feedforward network transforms each position independently!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\ngelu = NewGELU()\nff = FeedForward(n_embd=768, n_inner=3072)\n\nx_test = torch.randn(2, 10, 768)\noutput = ff(x_test)\n\nprint('\u2713 GELU implemented')\nprint('\u2713 FeedForward implemented')\nprint(f'\u2713 Shape preserved: {x_test.shape} \u2192 {output.shape}')\nprint('\\n\u2705 Basic functionality verified!')\nprint('\\nRun \"python -m pytest test_feedforward.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}