{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Layer Normalization & Residuals - Interactive Notebook\n",
    "\n",
    "Explore layer normalization and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from solution import ResidualConnection\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Layer Normalization Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input with varying scales\n",
    "n_embd = 768\n",
    "x = torch.randn(1, 5, n_embd)\n",
    "\n",
    "# Scale different positions differently\n",
    "x[0, 0, :] *= 10   # Large values\n",
    "x[0, 1, :] *= 5\n",
    "x[0, 2, :] *= 1\n",
    "x[0, 3, :] *= 0.5\n",
    "x[0, 4, :] *= 0.1  # Small values\n",
    "\n",
    "# Apply layer normalization\n",
    "ln = nn.LayerNorm(n_embd)\n",
    "x_normalized = ln(x)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Before normalization - values\n",
    "axes[0, 0].boxplot([x[0, i, :].detach().numpy() for i in range(5)], labels=[f'Pos {i}' for i in range(5)])\n",
    "axes[0, 0].set_title('Before LayerNorm: Value Distribution')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# After normalization - values\n",
    "axes[0, 1].boxplot([x_normalized[0, i, :].detach().numpy() for i in range(5)], labels=[f'Pos {i}' for i in range(5)])\n",
    "axes[0, 1].set_title('After LayerNorm: Value Distribution')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Mean and std before\n",
    "means_before = x.mean(dim=-1).squeeze().detach().numpy()\n",
    "stds_before = x.std(dim=-1).squeeze().detach().numpy()\n",
    "axes[1, 0].bar(range(5), means_before, alpha=0.7, label='Mean')\n",
    "axes[1, 0].bar(range(5), stds_before, alpha=0.7, label='Std')\n",
    "axes[1, 0].set_title('Before LayerNorm: Mean & Std per Position')\n",
    "axes[1, 0].set_xlabel('Position')\n",
    "axes[1, 0].set_xticks(range(5))\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Mean and std after\n",
    "means_after = x_normalized.mean(dim=-1).squeeze().detach().numpy()\n",
    "stds_after = x_normalized.std(dim=-1, unbiased=False).squeeze().detach().numpy()\n",
    "axes[1, 1].bar(range(5), means_after, alpha=0.7, label='Mean')\n",
    "axes[1, 1].bar(range(5), stds_after, alpha=0.7, label='Std')\n",
    "axes[1, 1].set_title('After LayerNorm: Mean ≈ 0, Std ≈ 1')\n",
    "axes[1, 1].set_xlabel('Position')\n",
    "axes[1, 1].set_xticks(range(5))\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Target Mean')\n",
    "axes[1, 1].axhline(y=1, color='g', linestyle='--', alpha=0.5, label='Target Std')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Layer Normalization ensures mean ≈ 0 and std ≈ 1 for each position!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare With and Without Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple sublayer (linear transformation)\n",
    "n_embd = 64\n",
    "linear = nn.Linear(n_embd, n_embd)\n",
    "sublayer = lambda x: linear(x)\n",
    "\n",
    "x = torch.randn(1, 10, n_embd)\n",
    "\n",
    "# Without residual (just sublayer)\n",
    "ln = nn.LayerNorm(n_embd)\n",
    "output_no_residual = sublayer(ln(x))\n",
    "\n",
    "# With residual\n",
    "res = ResidualConnection(n_embd=n_embd, dropout=0.0)\n",
    "res.eval()\n",
    "output_with_residual = res(x, sublayer)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Original input\n",
    "im1 = axes[0].imshow(x.squeeze().detach().T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "axes[0].set_title('Original Input')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Without residual\n",
    "im2 = axes[1].imshow(output_no_residual.squeeze().detach().T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "axes[1].set_title('Without Residual')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# With residual\n",
    "im3 = axes[2].imshow(output_with_residual.squeeze().detach().T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "axes[2].set_title('With Residual (Preserves Original)')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Dimension')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Residual connection preserves information from the input!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow with and without residual\n",
    "def measure_gradient_flow(use_residual=True, n_layers=10):\n",
    "    \"\"\"Measure how gradients flow through multiple layers.\"\"\"\n",
    "    n_embd = 64\n",
    "    x = torch.randn(1, 5, n_embd, requires_grad=True)\n",
    "    \n",
    "    output = x\n",
    "    layers = []\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        linear = nn.Linear(n_embd, n_embd)\n",
    "        layers.append(linear)\n",
    "        \n",
    "        if use_residual:\n",
    "            res = ResidualConnection(n_embd=n_embd, dropout=0.0)\n",
    "            output = res(output, lambda x: linear(x))\n",
    "        else:\n",
    "            ln = nn.LayerNorm(n_embd)\n",
    "            output = linear(ln(output))\n",
    "    \n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Measure gradient magnitude at each layer\n",
    "    grad_magnitudes = [layer.weight.grad.abs().mean().item() for layer in layers]\n",
    "    \n",
    "    return grad_magnitudes\n",
    "\n",
    "# Measure gradients\n",
    "n_layers = 10\n",
    "grads_with_residual = measure_gradient_flow(use_residual=True, n_layers=n_layers)\n",
    "grads_without_residual = measure_gradient_flow(use_residual=False, n_layers=n_layers)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(n_layers), grads_with_residual, 'o-', label='With Residual', linewidth=2)\n",
    "plt.plot(range(n_layers), grads_without_residual, 's-', label='Without Residual', linewidth=2)\n",
    "plt.xlabel('Layer Index (0 = closest to input)')\n",
    "plt.ylabel('Average Gradient Magnitude')\n",
    "plt.title('Gradient Flow Through Deep Network')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print('Residual connections help gradients flow more uniformly!')\n",
    "print(f'Without residual, gradients can vanish in early layers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pre-Norm vs Post-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pre-norm (GPT-2) vs post-norm (original Transformer)\n",
    "n_embd = 64\n",
    "linear = nn.Linear(n_embd, n_embd)\n",
    "ln = nn.LayerNorm(n_embd)\n",
    "\n",
    "x = torch.randn(1, 5, n_embd)\n",
    "\n",
    "# Pre-norm: x + sublayer(ln(x))\n",
    "pre_norm_output = x + linear(ln(x))\n",
    "\n",
    "# Post-norm: ln(x + sublayer(x))\n",
    "post_norm_output = ln(x + linear(x))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Input\n",
    "im1 = axes[0].imshow(x.squeeze().detach().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[0].set_title('Input')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Pre-norm (GPT-2)\n",
    "im2 = axes[1].imshow(pre_norm_output.squeeze().detach().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[1].set_title('Pre-Norm (GPT-2)')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Post-norm (Original Transformer)\n",
    "im3 = axes[2].imshow(post_norm_output.squeeze().detach().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[2].set_title('Post-Norm (Original Transformer)')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Dimension')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Pre-norm (GPT-2): Normalizes BEFORE sublayer')\n",
    "print('Post-norm (Original): Normalizes AFTER sublayer')\n",
    "print('\\nPre-norm is more stable for deep networks!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "res = ResidualConnection(n_embd=768, dropout=0.1)\n",
    "linear = nn.Linear(768, 768)\n",
    "sublayer = lambda x: linear(x)\n",
    "\n",
    "x_test = torch.randn(2, 10, 768)\n",
    "output = res(x_test, sublayer)\n",
    "\n",
    "print('✓ ResidualConnection implemented')\n",
    "print('✓ Layer normalization applied')\n",
    "print(f'✓ Shape preserved: {x_test.shape} → {output.shape}')\n",
    "print('✓ Residual connection working')\n",
    "print('\\n✅ Basic functionality verified!')\n",
    "print('\\nRun \"python -m pytest test_layer_norm.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
