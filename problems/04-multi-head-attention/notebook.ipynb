{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Multi-Head Attention - Interactive Notebook\n",
    "\n",
    "This notebook explores how multiple attention heads capture different patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Import your solution\n",
    "from solution import MultiHeadAttention\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi-Head Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention\n",
    "n_embd = 768\n",
    "n_head = 12  # GPT-2 small configuration\n",
    "\n",
    "mha = MultiHeadAttention(n_embd=n_embd, n_head=n_head, dropout=0.0)\n",
    "mha.eval()\n",
    "\n",
    "print(mha)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Embedding dimension: {n_embd}\")\n",
    "print(f\"  Number of heads: {n_head}\")\n",
    "print(f\"  Head dimension: {n_embd // n_head}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, n_embd)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mha(x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"✓ Shape preserved: {x.shape} → {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attention Patterns for Each Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights for each head\n",
    "# We'll need to modify the forward pass temporarily to capture attention weights\n",
    "\n",
    "class MultiHeadAttentionWithWeights(MultiHeadAttention):\n",
    "    \"\"\"Modified version that returns attention weights.\"\"\"\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Split into heads\n",
    "        q = q.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        out = self.c_proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        \n",
    "        return out, attn_weights  # Also return weights\n",
    "\n",
    "# Create modified version\n",
    "mha_with_weights = MultiHeadAttentionWithWeights(n_embd=n_embd, n_head=n_head, dropout=0.0)\n",
    "mha_with_weights.load_state_dict(mha.state_dict())  # Copy weights\n",
    "mha_with_weights.eval()\n",
    "\n",
    "# Generate sample input\n",
    "seq_len = 12\n",
    "x = torch.randn(1, seq_len, n_embd)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = mha_with_weights(x)\n",
    "\n",
    "# attn_weights shape: (batch=1, n_head=12, seq_len=12, seq_len=12)\n",
    "attn_weights = attn_weights.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Plot all 12 heads\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(n_head):\n",
    "    weights = attn_weights[head_idx].numpy()\n",
    "    \n",
    "    im = axes[head_idx].imshow(weights, cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[head_idx].set_title(f'Head {head_idx}', fontsize=11)\n",
    "    axes[head_idx].set_xlabel('Key')\n",
    "    axes[head_idx].set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Attention Patterns for Each Head', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Different heads learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Single-Head vs Multi-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single-head and multi-head versions\n",
    "n_embd_small = 128\n",
    "seq_len = 8\n",
    "\n",
    "single_head = MultiHeadAttention(n_embd=n_embd_small, n_head=1, dropout=0.0)\n",
    "multi_head = MultiHeadAttention(n_embd=n_embd_small, n_head=4, dropout=0.0)\n",
    "\n",
    "single_head.eval()\n",
    "multi_head.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, n_embd_small)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_single = single_head(x)\n",
    "    out_multi = multi_head(x)\n",
    "\n",
    "# Plot outputs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "im0 = axes[0].imshow(x.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "axes[0].set_title('Input', fontsize=12)\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(out_single.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "axes[1].set_title('Output (Single Head)', fontsize=12)\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2 = axes[2].imshow(out_multi.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "axes[2].set_title('Output (4 Heads)', fontsize=12)\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Multi-head attention creates richer representations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Specialization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what different heads might be learning\n",
    "# We'll compute attention statistics for each head\n",
    "\n",
    "seq_len = 16\n",
    "x = torch.randn(1, seq_len, n_embd)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, attn_weights = mha_with_weights(x)\n",
    "\n",
    "attn_weights = attn_weights.squeeze(0)  # (n_head, seq_len, seq_len)\n",
    "\n",
    "# Compute statistics for each head\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Entropy (how diffuse is the attention?)\n",
    "def entropy(probs):\n",
    "    return -(probs * torch.log(probs + 1e-9)).sum(dim=-1).mean()\n",
    "\n",
    "entropies = [entropy(attn_weights[h]).item() for h in range(n_head)]\n",
    "axes[0, 0].bar(range(n_head), entropies, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Head')\n",
    "axes[0, 0].set_ylabel('Entropy')\n",
    "axes[0, 0].set_title('Attention Entropy by Head (higher = more diffuse)')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Max attention weight (how focused?)\n",
    "max_weights = [attn_weights[h].max(dim=-1)[0].mean().item() for h in range(n_head)]\n",
    "axes[0, 1].bar(range(n_head), max_weights, color='coral')\n",
    "axes[0, 1].set_xlabel('Head')\n",
    "axes[0, 1].set_ylabel('Average Max Weight')\n",
    "axes[0, 1].set_title('Attention Focus by Head (higher = more focused)')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Local vs global attention (average attention distance)\n",
    "def attention_distance(weights):\n",
    "    \"\"\"Compute average distance of attention.\"\"\"\n",
    "    positions = torch.arange(seq_len).float().view(1, -1)\n",
    "    distances = []\n",
    "    for i in range(seq_len):\n",
    "        expected_pos = (weights[i] * positions).sum()\n",
    "        dist = abs(expected_pos - i).item()\n",
    "        distances.append(dist)\n",
    "    return np.mean(distances)\n",
    "\n",
    "distances = [attention_distance(attn_weights[h]) for h in range(n_head)]\n",
    "axes[1, 0].bar(range(n_head), distances, color='green')\n",
    "axes[1, 0].set_xlabel('Head')\n",
    "axes[1, 0].set_ylabel('Average Attention Distance')\n",
    "axes[1, 0].set_title('Local vs Global Attention (lower = more local)')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Diagonal dominance (self-attention strength)\n",
    "diagonal_weights = [attn_weights[h].diag().mean().item() for h in range(n_head)]\n",
    "axes[1, 1].bar(range(n_head), diagonal_weights, color='purple')\n",
    "axes[1, 1].set_xlabel('Head')\n",
    "axes[1, 1].set_ylabel('Average Diagonal Weight')\n",
    "axes[1, 1].set_title('Self-Attention Strength (higher = more self-focused)')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Different heads show different behaviors!\")\n",
    "print(\"Some are local, some are global, some are diffuse, some are focused.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Head Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how heads are split and concatenated\n",
    "n_embd_vis = 64\n",
    "n_head_vis = 4\n",
    "seq_len = 8\n",
    "\n",
    "mha_vis = MultiHeadAttentionWithWeights(n_embd=n_embd_vis, n_head=n_head_vis, dropout=0.0)\n",
    "mha_vis.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, n_embd_vis)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get Q, K, V\n",
    "    qkv = mha_vis.c_attn(x)\n",
    "    q, k, v = qkv.split(n_embd_vis, dim=2)\n",
    "    \n",
    "    # Before splitting into heads\n",
    "    q_before = q.squeeze()\n",
    "    \n",
    "    # After splitting into heads\n",
    "    head_dim = n_embd_vis // n_head_vis\n",
    "    q_heads = q.view(1, seq_len, n_head_vis, head_dim).transpose(1, 2).squeeze(0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before split\n",
    "im1 = axes[0].imshow(q_before.T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[0].set_title('Query Before Head Split', fontsize=12)\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Embedding Dimension (64)')\n",
    "axes[0].axhline(y=15.5, color='yellow', linestyle='--', linewidth=2, label='Head boundaries')\n",
    "axes[0].axhline(y=31.5, color='yellow', linestyle='--', linewidth=2)\n",
    "axes[0].axhline(y=47.5, color='yellow', linestyle='--', linewidth=2)\n",
    "axes[0].legend()\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# After split (show all heads stacked)\n",
    "q_heads_vis = q_heads.reshape(n_head_vis * seq_len, head_dim).T\n",
    "im2 = axes[1].imshow(q_heads_vis.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[1].set_title('Query After Head Split (heads stacked)', fontsize=12)\n",
    "axes[1].set_xlabel('Position (Head 0 | Head 1 | Head 2 | Head 3)')\n",
    "axes[1].set_ylabel('Head Dimension (16)')\n",
    "\n",
    "# Add vertical lines to separate heads\n",
    "for i in range(1, n_head_vis):\n",
    "    axes[1].axvline(x=i*seq_len - 0.5, color='yellow', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Embedding split: {n_embd_vis} dimensions → {n_head_vis} heads × {head_dim} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Number of Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different numbers of heads\n",
    "n_embd_test = 256\n",
    "seq_len = 10\n",
    "head_configs = [1, 2, 4, 8, 16]\n",
    "\n",
    "x = torch.randn(1, seq_len, n_embd_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(head_configs), figsize=(18, 4))\n",
    "\n",
    "for idx, n_heads in enumerate(head_configs):\n",
    "    mha_test = MultiHeadAttention(n_embd=n_embd_test, n_head=n_heads, dropout=0.0)\n",
    "    mha_test.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = mha_test(x)\n",
    "    \n",
    "    im = axes[idx].imshow(output.squeeze().T.numpy(), aspect='auto', cmap='viridis')\n",
    "    axes[idx].set_title(f'{n_heads} Head(s)\\n(head_dim={n_embd_test//n_heads})', fontsize=11)\n",
    "    axes[idx].set_xlabel('Position')\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"More heads allow the model to attend to different aspects simultaneously!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter counts\n",
    "configs = [\n",
    "    (768, 1, \"Single head\"),\n",
    "    (768, 2, \"2 heads\"),\n",
    "    (768, 4, \"4 heads\"),\n",
    "    (768, 6, \"6 heads\"),\n",
    "    (768, 12, \"12 heads (GPT-2)\"),\n",
    "]\n",
    "\n",
    "param_counts = []\n",
    "labels = []\n",
    "\n",
    "for n_embd, n_head, label in configs:\n",
    "    mha = MultiHeadAttention(n_embd=n_embd, n_head=n_head)\n",
    "    params = sum(p.numel() for p in mha.parameters())\n",
    "    param_counts.append(params)\n",
    "    labels.append(label)\n",
    "    print(f\"{label:25s}: {params:,} parameters\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(labels)), param_counts, color='steelblue')\n",
    "plt.xticks(range(len(labels)), labels, rotation=15, ha='right')\n",
    "plt.ylabel('Number of Parameters')\n",
    "plt.title('Parameter Count vs Number of Heads (n_embd=768)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Parameter count is the SAME regardless of number of heads!\")\n",
    "print(\"Multi-head attention doesn't add parameters, it splits computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "mha = MultiHeadAttention(n_embd=768, n_head=12, dropout=0.1)\n",
    "x = torch.randn(2, 10, 768)\n",
    "output = mha(x)\n",
    "\n",
    "print(\"✓ Module initializes correctly\")\n",
    "print(f\"✓ Forward pass works: {x.shape} → {output.shape}\")\n",
    "print(f\"✓ Shape preserved: {x.shape == output.shape}\")\n",
    "print(f\"✓ Head dimension: {mha.head_dim} = {768//12}\")\n",
    "print(\"\\n✅ Basic functionality verified!\")\n",
    "print(\"\\nRun 'python -m pytest test_multi_head_attention.py -v' for complete validation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
