{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Causal Masking - Interactive Notebook\n",
    "\n",
    "Explore how causal masking enables autoregressive language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from solution import CausalMultiHeadAttention\n",
    "# We'll also import non-causal version for comparison\n",
    "from solution import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention module and extract mask\n",
    "attn = CausalMultiHeadAttention(n_embd=64, n_head=4, n_positions=12)\n",
    "\n",
    "mask = attn.bias.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(mask, annot=True, fmt='.0f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Can Attend?'},\n",
    "            xticklabels=range(12), yticklabels=range(12))\n",
    "plt.title('Causal Mask (Lower Triangular)', fontsize=14)\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('1 = Can attend, 0 = Cannot attend (masked)')\n",
    "print('Each position can only see itself and previous positions!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: Causal vs Bidirectional Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to extract attention weights\n",
    "class CausalAttnWithWeights(CausalMultiHeadAttention):\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        q = q.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.bias[:, :, :seq_len, :seq_len] == 0, float(\"-inf\")\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        out = self.c_proj(out)\n",
    "        return out, attn_weights\n",
    "\n",
    "class BiAttnWithWeights(MultiHeadAttention):\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        q = q.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        out = self.c_proj(out)\n",
    "        return out, attn_weights\n",
    "\n",
    "# Create both types\n",
    "n_embd, n_head, seq_len = 128, 4, 10\n",
    "causal_attn = CausalAttnWithWeights(n_embd=n_embd, n_head=n_head, dropout=0.0)\n",
    "bidir_attn = BiAttnWithWeights(n_embd=n_embd, n_head=n_head, dropout=0.0)\n",
    "\n",
    "causal_attn.eval()\n",
    "bidir_attn.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, n_embd)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, causal_weights = causal_attn(x)\n",
    "    _, bidir_weights = bidir_attn(x)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bidirectional (head 0)\n",
    "im1 = axes[0].imshow(bidir_weights[0, 0].numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title('Bidirectional Attention (BERT-style)', fontsize=13)\n",
    "axes[0].set_xlabel('Attending to Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Causal (head 0)\n",
    "im2 = axes[1].imshow(causal_weights[0, 0].numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title('Causal Attention (GPT-style)', fontsize=13)\n",
    "axes[1].set_xlabel('Attending to Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Notice the triangular pattern in causal attention!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Generation Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate text generation\n",
    "# Sentence: \"The cat sat on the\"\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\"]\n",
    "seq_len = len(tokens)\n",
    "\n",
    "causal_attn_viz = CausalAttnWithWeights(n_embd=64, n_head=1, dropout=0.0)\n",
    "causal_attn_viz.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, weights = causal_attn_viz(x)\n",
    "\n",
    "weights = weights.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.heatmap(weights, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=tokens, yticklabels=tokens,\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Autoregressive Attention Pattern\\n(Each position only sees past)', fontsize=14)\n",
    "plt.xlabel('Attending to Word')\n",
    "plt.ylabel('Current Word (predicting next)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nHow each word predicts the next:')\n",
    "for i, token in enumerate(tokens):\n",
    "    context = tokens[:i+1]\n",
    "    print(f'{i}. \"{token}\" can see: {\", \".join(context)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weights Per Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how attention changes by position\n",
    "seq_len = 8\n",
    "causal_attn = CausalAttnWithWeights(n_embd=64, n_head=1, dropout=0.0)\n",
    "causal_attn.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, weights = causal_attn(x)\n",
    "\n",
    "weights_np = weights.squeeze().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    # Show attention distribution for this position\n",
    "    attn_dist = weights_np[pos]\n",
    "    \n",
    "    colors = ['green' if i <= pos else 'red' for i in range(seq_len)]\n",
    "    axes[pos].bar(range(seq_len), attn_dist, color=colors, alpha=0.7)\n",
    "    axes[pos].set_title(f'Position {pos}', fontsize=11)\n",
    "    axes[pos].set_xlabel('Can attend to')\n",
    "    axes[pos].set_ylabel('Attention Weight')\n",
    "    axes[pos].set_ylim([0, 1])\n",
    "    axes[pos].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add vertical line at current position\n",
    "    axes[pos].axvline(x=pos, color='blue', linestyle='--', linewidth=2, label='Current')\n",
    "    if pos == 0:\n",
    "        axes[pos].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Green = Can attend | Red = Masked (future positions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect on Different Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show causal masking across multiple heads\n",
    "seq_len = 12\n",
    "n_head = 6\n",
    "causal_attn_multi = CausalAttnWithWeights(n_embd=192, n_head=n_head, dropout=0.0)\n",
    "causal_attn_multi.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, 192)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, weights = causal_attn_multi(x)\n",
    "\n",
    "weights = weights.squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head in range(n_head):\n",
    "    im = axes[head].imshow(weights[head].numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[head].set_title(f'Head {head} (Causal)', fontsize=12)\n",
    "    axes[head].set_xlabel('Key')\n",
    "    axes[head].set_ylabel('Query')\n",
    "    plt.colorbar(im, ax=axes[head])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('All heads respect the causal mask!')\n",
    "print('But each learns different patterns within the allowed region.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Masking Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key properties of causal masking\n",
    "seq_len = 10\n",
    "causal_attn = CausalAttnWithWeights(n_embd=128, n_head=4, dropout=0.0)\n",
    "causal_attn.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, weights = causal_attn(x)\n",
    "\n",
    "print('=== Causal Masking Verification ===')\n",
    "print()\n",
    "\n",
    "# Check 1: Future positions are 0\n",
    "future_is_zero = True\n",
    "for head in range(4):\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i+1, seq_len):\n",
    "            if weights[0, head, i, j] != 0:\n",
    "                future_is_zero = False\n",
    "\n",
    "print(f'✓ Future positions are masked (zero): {future_is_zero}')\n",
    "\n",
    "# Check 2: Past positions can have non-zero attention\n",
    "past_has_attention = (weights[0, :, 1:, :-1] > 0).any().item()\n",
    "print(f'✓ Past positions have attention: {past_has_attention}')\n",
    "\n",
    "# Check 3: Attention weights sum to 1\n",
    "row_sums = weights.sum(dim=-1)\n",
    "sums_to_one = torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6)\n",
    "print(f'✓ Attention weights sum to 1: {sums_to_one}')\n",
    "\n",
    "# Check 4: First position only attends to itself\n",
    "first_pos_self = torch.allclose(\n",
    "    weights[0, :, 0, 0], \n",
    "    torch.ones(4), \n",
    "    atol=1e-6\n",
    ")\n",
    "print(f'✓ First position only self-attends: {first_pos_self}')\n",
    "\n",
    "# Check 5: Last position can attend to all\n",
    "last_pos_all = (weights[0, :, -1, :] > 0).all().item()\n",
    "print(f'✓ Last position attends to all: {last_pos_all}')\n",
    "\n",
    "print()\n",
    "print('✅ All causal masking properties verified!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different sequence lengths\n",
    "causal_attn = CausalAttnWithWeights(n_embd=64, n_head=2, n_positions=20, dropout=0.0)\n",
    "causal_attn.eval()\n",
    "\n",
    "seq_lengths = [4, 8, 12, 16]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, seq_len in enumerate(seq_lengths):\n",
    "    x = torch.randn(1, seq_len, 64)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, weights = causal_attn(x)\n",
    "    \n",
    "    im = axes[idx].imshow(weights[0, 0].numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    axes[idx].set_title(f'Sequence Length: {seq_len}', fontsize=12)\n",
    "    axes[idx].set_xlabel('Key Position')\n",
    "    axes[idx].set_ylabel('Query Position')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Causal masking adapts to any sequence length!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "attn = CausalMultiHeadAttention(n_embd=768, n_head=12, n_positions=1024, dropout=0.1)\n",
    "x = torch.randn(2, 10, 768)\n",
    "output = attn(x)\n",
    "\n",
    "print('✓ Module initializes correctly')\n",
    "print(f'✓ Forward pass works: {x.shape} → {output.shape}')\n",
    "print(f'✓ Causal mask registered: {\"bias\" in dict(attn.named_buffers())}')\n",
    "print(f'✓ Mask is lower triangular: {torch.allclose(attn.bias, torch.tril(attn.bias))}')\n",
    "print('\\n✅ Basic functionality verified!')\n",
    "print('\\nRun \"python -m pytest test_causal_masking.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-gpt2)",
   "language": "python",
   "name": "learn-gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
