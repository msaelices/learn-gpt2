{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8: Complete Transformer Block - Interactive Notebook\n",
    "\n",
    "Explore the full transformer block combining attention and feedforward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from solution import TransformerBlock\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Inspect Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer block\n",
    "n_embd, n_head, n_positions = 768, 12, 1024\n",
    "block = TransformerBlock(\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_positions=n_positions,\n",
    "    attn_pdrop=0.0,\n",
    "    resid_pdrop=0.0,\n",
    ")\n",
    "block.eval()\n",
    "\n",
    "print('Transformer Block Structure:')\n",
    "print(f'  Input dimension: {n_embd}')\n",
    "print(f'  Number of attention heads: {n_head}')\n",
    "print(f'  Head dimension: {n_embd // n_head}')\n",
    "print(f'  FFN inner dimension: {block.mlp.c_fc.out_features} (4x expansion)')\n",
    "print(f'  Max sequence length: {n_positions}')\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in block.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Information Flow Through Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track activations through the block\n",
    "seq_len = 10\n",
    "x = torch.randn(1, seq_len, n_embd)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Step by step through the block\n",
    "    x1_ln = block.ln_1(x)  # Layer norm 1\n",
    "    x1_attn = block.attn(x1_ln)  # Attention\n",
    "    x1_res = x + x1_attn  # Residual 1\n",
    "    \n",
    "    x2_ln = block.ln_2(x1_res)  # Layer norm 2\n",
    "    x2_mlp = block.mlp(x2_ln)  # Feedforward\n",
    "    x2_res = x1_res + x2_mlp  # Residual 2\n",
    "    \n",
    "    # Full forward pass\n",
    "    output = block(x)\n",
    "\n",
    "# Verify our manual computation matches\n",
    "assert torch.allclose(x2_res, output, atol=1e-5)\n",
    "\n",
    "# Visualize activations at each stage\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "stages = [\n",
    "    (x, 'Input'),\n",
    "    (x1_ln, 'After LN1'),\n",
    "    (x1_attn, 'Attention Output'),\n",
    "    (x1_res, 'After Residual 1'),\n",
    "    (x2_ln, 'After LN2'),\n",
    "    (x2_mlp, 'FFN Output'),\n",
    "    (x2_res, 'After Residual 2'),\n",
    "    (output, 'Final Output'),\n",
    "]\n",
    "\n",
    "for idx, (tensor, title) in enumerate(stages):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    # Show first 64 dimensions for visibility\n",
    "    im = ax.imshow(tensor.squeeze()[:, :64].T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Dimension (first 64)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Information flows through: Input → LN1 → Attn → Res1 → LN2 → FFN → Res2 → Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Residual Connection Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without residual connections\n",
    "x_test = torch.randn(1, 10, 64)\n",
    "\n",
    "# Small block for visualization\n",
    "small_block = TransformerBlock(\n",
    "    n_embd=64,\n",
    "    n_head=4,\n",
    "    n_positions=128,\n",
    "    attn_pdrop=0.0,\n",
    "    resid_pdrop=0.0,\n",
    ")\n",
    "small_block.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # With residual\n",
    "    output_with_residual = small_block(x_test)\n",
    "    \n",
    "    # Without residual (just the transformations)\n",
    "    x_ln1 = small_block.ln_1(x_test)\n",
    "    x_attn = small_block.attn(x_ln1)\n",
    "    x_ln2 = small_block.ln_2(x_attn)\n",
    "    output_no_residual = small_block.mlp(x_ln2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "im1 = axes[0].imshow(x_test.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[0].set_title('Input')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(output_no_residual.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[1].set_title('Without Residuals\\n(Pure Transformation)')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "im3 = axes[2].imshow(output_with_residual.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "axes[2].set_title('With Residuals\\n(Preserves Input Info)')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Dimension')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Residual connections help preserve information from input!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Single Block vs Stacked Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple blocks\n",
    "n_embd = 64\n",
    "blocks = nn.ModuleList([\n",
    "    TransformerBlock(\n",
    "        n_embd=n_embd,\n",
    "        n_head=4,\n",
    "        n_positions=128,\n",
    "        attn_pdrop=0.0,\n",
    "        resid_pdrop=0.0,\n",
    "    )\n",
    "    for _ in range(4)\n",
    "])\n",
    "\n",
    "for block in blocks:\n",
    "    block.eval()\n",
    "\n",
    "x = torch.randn(1, 10, n_embd)\n",
    "\n",
    "# Pass through blocks one by one\n",
    "outputs = [x]\n",
    "current = x\n",
    "with torch.no_grad():\n",
    "    for block in blocks:\n",
    "        current = block(current)\n",
    "        outputs.append(current)\n",
    "\n",
    "# Visualize progression\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "for i, (output, ax) in enumerate(zip(outputs, axes)):\n",
    "    im = ax.imshow(output.squeeze().T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "    if i == 0:\n",
    "        ax.set_title('Input')\n",
    "    else:\n",
    "        ax.set_title(f'After Block {i}')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Dimension')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Each block refines the representation!')\n",
    "print('GPT-2 small uses 12 blocks, medium uses 24, large uses 36, xl uses 48!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Gradient Flow Through Stacked Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient flow through multiple blocks\n",
    "n_blocks = 8\n",
    "n_embd = 64\n",
    "\n",
    "blocks = nn.ModuleList([\n",
    "    TransformerBlock(\n",
    "        n_embd=n_embd,\n",
    "        n_head=4,\n",
    "        n_positions=128,\n",
    "        attn_pdrop=0.0,\n",
    "        resid_pdrop=0.0,\n",
    "    )\n",
    "    for _ in range(n_blocks)\n",
    "])\n",
    "\n",
    "x = torch.randn(1, 5, n_embd, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "current = x\n",
    "for block in blocks:\n",
    "    current = block(current)\n",
    "\n",
    "# Backward pass\n",
    "loss = current.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Measure gradient magnitude at each block\n",
    "grad_magnitudes = []\n",
    "for i, block in enumerate(blocks):\n",
    "    # Check layer norm 1 gradient as proxy for block gradient\n",
    "    grad_mag = block.ln_1.weight.grad.abs().mean().item()\n",
    "    grad_magnitudes.append(grad_mag)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(n_blocks), grad_magnitudes)\n",
    "plt.xlabel('Block Index')\n",
    "plt.ylabel('Average Gradient Magnitude')\n",
    "plt.title('Gradient Flow Through Stacked Transformer Blocks')\n",
    "plt.axhline(y=np.mean(grad_magnitudes), color='r', linestyle='--', label='Mean')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'Gradient magnitudes relatively uniform: {np.std(grad_magnitudes):.6f} std dev')\n",
    "print('Residual connections enable stable gradient flow through deep networks!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter distribution\n",
    "block = TransformerBlock(\n",
    "    n_embd=768,\n",
    "    n_head=12,\n",
    "    n_positions=1024,\n",
    ")\n",
    "\n",
    "param_counts = {\n",
    "    'Attention (Q,K,V)': block.attn.c_attn.weight.numel() + block.attn.c_attn.bias.numel(),\n",
    "    'Attention (Output proj)': block.attn.c_proj.weight.numel() + block.attn.c_proj.bias.numel(),\n",
    "    'FFN (Expansion)': block.mlp.c_fc.weight.numel() + block.mlp.c_fc.bias.numel(),\n",
    "    'FFN (Projection)': block.mlp.c_proj.weight.numel() + block.mlp.c_proj.bias.numel(),\n",
    "    'LayerNorms': sum(p.numel() for p in block.ln_1.parameters()) + \n",
    "                  sum(p.numel() for p in block.ln_2.parameters()),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(param_counts.keys(), param_counts.values())\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Number of Parameters')\n",
    "plt.title('Parameter Distribution in Transformer Block')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total = sum(param_counts.values())\n",
    "print('\\nParameter breakdown:')\n",
    "for name, count in param_counts.items():\n",
    "    print(f'  {name}: {count:,} ({100*count/total:.1f}%)')\n",
    "print(f'\\nTotal: {total:,} parameters per block')\n",
    "print(f'\\nGPT-2 small (12 blocks): ~{total * 12 / 1e6:.1f}M parameters in transformer blocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "block = TransformerBlock(\n",
    "    n_embd=768,\n",
    "    n_head=12,\n",
    "    n_positions=1024,\n",
    ")\n",
    "\n",
    "x_test = torch.randn(2, 10, 768)\n",
    "output = block(x_test)\n",
    "\n",
    "print('✓ TransformerBlock implemented')\n",
    "print('✓ Two layer norms (pre-norm architecture)')\n",
    "print('✓ Causal multi-head attention')\n",
    "print('✓ Position-wise feedforward network')\n",
    "print('✓ Two residual connections')\n",
    "print(f'✓ Shape preserved: {x_test.shape} → {output.shape}')\n",
    "print('\\n✅ Complete transformer block working!')\n",
    "print('\\nRun \"python -m pytest test_transformer_block.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
