{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 9: GPT-2 Configuration - Interactive Notebook\n",
    "\n",
    "Explore GPT-2 configurations and understand model scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# When finished, uncomment the following sentence and import your own solution\n",
    "from solution import GPT2Config\n",
    "# from problem import GPT2Config\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Configurations for All Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all four GPT-2 model sizes\n",
    "small = GPT2Config.gpt2_small()\n",
    "medium = GPT2Config.gpt2_medium()\n",
    "large = GPT2Config.gpt2_large()\n",
    "xl = GPT2Config.gpt2_xl()\n",
    "\n",
    "configs = {\n",
    "    'Small (124M)': small,\n",
    "    'Medium (355M)': medium,\n",
    "    'Large (774M)': large,\n",
    "    'XL (1.5B)': xl,\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print('GPT-2 Model Configurations:')\n",
    "print('=' * 70)\n",
    "print(f\"{'Model':<20} {'Layers':<10} {'Emb Dim':<10} {'Heads':<10} {'Head Dim':<10}\")\n",
    "print('=' * 70)\n",
    "\n",
    "for name, config in configs.items():\n",
    "    head_dim = config.n_embd // config.n_head\n",
    "    print(f\"{name:<20} {config.n_layer:<10} {config.n_embd:<10} {config.n_head:<10} {head_dim:<10}\")\n",
    "\n",
    "print('\\nAll models share:')\n",
    "print(f'  Vocabulary size: {small.vocab_size:,}')\n",
    "print(f'  Context length: {small.n_positions:,} tokens')\n",
    "print(f'  FFN expansion: 4x (n_inner = 4 * n_embd)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "model_names = list(configs.keys())\n",
    "n_layers = [config.n_layer for config in configs.values()]\n",
    "n_embds = [config.n_embd for config in configs.values()]\n",
    "n_heads = [config.n_head for config in configs.values()]\n",
    "n_inners = [config.n_inner for config in configs.values()]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Number of layers\n",
    "axes[0, 0].bar(range(len(model_names)), n_layers, color='steelblue')\n",
    "axes[0, 0].set_xticks(range(len(model_names)))\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Number of Layers')\n",
    "axes[0, 0].set_title('Transformer Layers by Model Size')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(n_layers):\n",
    "    axes[0, 0].text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Embedding dimension\n",
    "axes[0, 1].bar(range(len(model_names)), n_embds, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(model_names)))\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Embedding Dimension')\n",
    "axes[0, 1].set_title('Embedding Dimension by Model Size')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(n_embds):\n",
    "    axes[0, 1].text(i, v + 30, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Number of heads\n",
    "axes[1, 0].bar(range(len(model_names)), n_heads, color='mediumseagreen')\n",
    "axes[1, 0].set_xticks(range(len(model_names)))\n",
    "axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Number of Attention Heads')\n",
    "axes[1, 0].set_title('Attention Heads by Model Size')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(n_heads):\n",
    "    axes[1, 0].text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: FFN inner dimension\n",
    "axes[1, 1].bar(range(len(model_names)), n_inners, color='mediumpurple')\n",
    "axes[1, 1].set_xticks(range(len(model_names)))\n",
    "axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('FFN Inner Dimension')\n",
    "axes[1, 1].set_title('Feedforward Inner Dimension by Model Size')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(n_inners):\n",
    "    axes[1, 1].text(i, v + 100, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Model size increases through scaling all three dimensions:')\n",
    "print('  - More layers (depth)')\n",
    "print('  - Larger embedding dimension (width)')\n",
    "print('  - More attention heads (parallelism)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Parameter Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(config):\n",
    "    \"\"\"Estimate total parameters for a GPT-2 configuration.\"\"\"\n",
    "    # Token embeddings\n",
    "    vocab_params = config.vocab_size * config.n_embd\n",
    "    \n",
    "    # Position embeddings\n",
    "    pos_params = config.n_positions * config.n_embd\n",
    "    \n",
    "    # Attention: Q,K,V projection (3 * n_embd * n_embd) + output projection (n_embd * n_embd)\n",
    "    attn_params = 4 * config.n_embd * config.n_embd\n",
    "    attn_bias = 4 * config.n_embd  # Biases\n",
    "    \n",
    "    # FFN: two linear layers\n",
    "    ffn_params = 2 * config.n_embd * config.n_inner\n",
    "    ffn_bias = config.n_embd + config.n_inner  # Biases\n",
    "    \n",
    "    # LayerNorm: 2 per layer (ln_1 and ln_2)\n",
    "    ln_params = 2 * (2 * config.n_embd)  # weight + bias\n",
    "    \n",
    "    # Per-layer parameters\n",
    "    layer_params = attn_params + attn_bias + ffn_params + ffn_bias + ln_params\n",
    "    total_layer_params = config.n_layer * layer_params\n",
    "    \n",
    "    # Final layer norm\n",
    "    final_ln = 2 * config.n_embd\n",
    "    \n",
    "    # Total (note: lm_head shares weights with token embeddings)\n",
    "    total = vocab_params + pos_params + total_layer_params + final_ln\n",
    "    \n",
    "    return {\n",
    "        'embeddings': vocab_params + pos_params,\n",
    "        'layers': total_layer_params,\n",
    "        'final_ln': final_ln,\n",
    "        'total': total,\n",
    "    }\n",
    "\n",
    "# Calculate for all models\n",
    "print('Parameter Counts (Approximate):')\n",
    "print('=' * 70)\n",
    "print(f\"{'Model':<20} {'Embeddings':<15} {'Layers':<15} {'Total':<15}\")\n",
    "print('=' * 70)\n",
    "\n",
    "param_counts = {}\n",
    "for name, config in configs.items():\n",
    "    params = estimate_parameters(config)\n",
    "    param_counts[name] = params['total']\n",
    "    \n",
    "    emb_str = f\"{params['embeddings']/1e6:.1f}M\"\n",
    "    layers_str = f\"{params['layers']/1e6:.1f}M\"\n",
    "    total_str = f\"{params['total']/1e6:.1f}M\"\n",
    "    \n",
    "    print(f\"{name:<20} {emb_str:<15} {layers_str:<15} {total_str:<15}\")\n",
    "\n",
    "print('\\nNote: Actual counts may vary slightly due to implementation details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Parameter Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter breakdown for each model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, config) in enumerate(configs.items()):\n",
    "    params = estimate_parameters(config)\n",
    "    \n",
    "    # Create pie chart\n",
    "    sizes = [params['embeddings'], params['layers'], params['final_ln']]\n",
    "    labels = ['Embeddings', 'Transformer Layers', 'Final LN']\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    explode = (0.05, 0, 0)\n",
    "    \n",
    "    axes[idx].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                  autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    axes[idx].set_title(f'{name}\\nTotal: {params[\"total\"]/1e6:.0f}M params')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Most parameters are in the transformer layers!')\n",
    "print('Embeddings are a smaller fraction in larger models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Head Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate head dimension for each model\n",
    "head_dims = [config.n_embd // config.n_head for config in configs.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_names)), head_dims, color='teal')\n",
    "plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')\n",
    "plt.ylabel('Head Dimension (n_embd / n_head)')\n",
    "plt.title('Attention Head Dimension by Model Size')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(head_dims):\n",
    "    plt.text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Head dimension: n_embd // n_head')\n",
    "print('\\nHead dimensions for each model:')\n",
    "for name, head_dim in zip(model_names, head_dims):\n",
    "    print(f'  {name}: {head_dim}')\n",
    "print('\\nAll head dimensions are 64! This is a design choice for consistency.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom tiny model for experimentation\n",
    "tiny = GPT2Config(\n",
    "    n_embd=256,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    n_positions=512,  # Shorter context\n",
    ")\n",
    "\n",
    "print('Custom Tiny GPT-2 Configuration:')\n",
    "print(f'  Layers: {tiny.n_layer}')\n",
    "print(f'  Embedding dim: {tiny.n_embd}')\n",
    "print(f'  Attention heads: {tiny.n_head}')\n",
    "print(f'  Head dimension: {tiny.n_embd // tiny.n_head}')\n",
    "print(f'  FFN inner dim: {tiny.n_inner}')\n",
    "print(f'  Context length: {tiny.n_positions}')\n",
    "print(f'  Vocabulary: {tiny.vocab_size:,}')\n",
    "\n",
    "tiny_params = estimate_parameters(tiny)\n",
    "print(f'\\nEstimated parameters: {tiny_params[\"total\"]/1e6:.1f}M')\n",
    "print('\\nThis tiny model is great for fast experimentation!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Configuration Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that validation works\n",
    "print('Testing configuration validation:\\n')\n",
    "\n",
    "# Valid configuration\n",
    "try:\n",
    "    config = GPT2Config(n_embd=768, n_head=12)\n",
    "    print('\u2713 Valid config (768 \u00f7 12 = 64): Success')\n",
    "except AssertionError as e:\n",
    "    print(f'\u2717 Unexpected error: {e}')\n",
    "\n",
    "# Invalid configuration: n_embd not divisible by n_head\n",
    "try:\n",
    "    config = GPT2Config(n_embd=770, n_head=12)\n",
    "    print('\u2717 Invalid config (770 \u00f7 12 = 64.16...) should have failed!')\n",
    "except AssertionError as e:\n",
    "    print(f'\u2713 Invalid config correctly rejected: {e}')\n",
    "\n",
    "print('\\nValidation ensures n_embd is always divisible by n_head!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "config = GPT2Config()\n",
    "\n",
    "print('\u2713 GPT2Config implemented')\n",
    "print('\u2713 All parameters stored correctly')\n",
    "print('\u2713 Validation working (n_embd divisible by n_head)')\n",
    "print('\u2713 Computed n_inner = 4 * n_embd')\n",
    "print('\u2713 All four preset configurations available')\n",
    "print('\\n\u2705 Configuration class complete!')\n",
    "print('\\nRun \"python -m pytest test_config.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}