{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 10: Full GPT-2 Model Assembly - Interactive Notebook\n",
    "\n",
    "Explore the complete GPT-2 architecture and understand how all components work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../09-gpt2-config')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from solution import GPT2Model, GPT2Config\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GPT-2 Models of Different Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for different sizes\n",
    "configs = {\n",
    "    'Small': GPT2Config.gpt2_small(),\n",
    "    'Medium': GPT2Config.gpt2_medium(),\n",
    "    'Large': GPT2Config.gpt2_large(),\n",
    "    'XL': GPT2Config.gpt2_xl(),\n",
    "}\n",
    "\n",
    "# Create a small model for experiments\n",
    "small_config = GPT2Config.gpt2_small()\n",
    "model = GPT2Model(small_config)\n",
    "\n",
    "print(f'Created GPT-2 Small model with {small_config.n_layer} layers')\n",
    "print(f'Embedding dimension: {small_config.n_embd}')\n",
    "print(f'Attention heads: {small_config.n_head}')\n",
    "print(f'FFN inner dimension: {small_config.n_inner}')\n",
    "print(f'Vocabulary size: {small_config.vocab_size:,}')\n",
    "print(f'Context length: {small_config.n_positions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "input_ids = torch.randint(0, small_config.vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f'Input shape: {input_ids.shape}')\n",
    "print(f'Sample tokens: {input_ids[0, :10].tolist()}')\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "\n",
    "print(f'\\nOutput logits shape: {logits.shape}')\n",
    "print(f'Expected: (batch_size={batch_size}, seq_len={seq_len}, vocab_size={small_config.vocab_size})')\n",
    "\n",
    "# Get probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(f'\\nProbabilities shape: {probs.shape}')\n",
    "print(f'Probabilities sum to 1: {probs[0, 0].sum():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters by component\n",
    "param_breakdown = model.get_num_params_by_component()\n",
    "\n",
    "# Print breakdown\n",
    "print('Parameter Breakdown:')\n",
    "print('=' * 60)\n",
    "for component, count in param_breakdown.items():\n",
    "    if component == 'total':\n",
    "        print('=' * 60)\n",
    "    pct = (count / param_breakdown['total'] * 100) if param_breakdown['total'] > 0 else 0\n",
    "    print(f'{component:25s}: {count:>12,} ({pct:>5.1f}%)')\n",
    "\n",
    "# Visualize parameter distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart\n",
    "components = ['embeddings', 'transformer_blocks', 'final_ln']\n",
    "sizes = [param_breakdown[c] for c in components]\n",
    "labels = ['Embeddings\\n(Token + Pos)', 'Transformer Blocks\\n(12 layers)', 'Final LayerNorm']\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "ax1.set_title(f'GPT-2 Small Parameter Distribution\\nTotal: {param_breakdown[\"total\"]/1e6:.1f}M parameters')\n",
    "\n",
    "# Bar chart\n",
    "component_names = ['Embeddings', 'Transformer\\nBlocks', 'Final\\nLayerNorm']\n",
    "ax2.bar(range(len(components)), [s/1e6 for s in sizes], color=colors)\n",
    "ax2.set_xticks(range(len(components)))\n",
    "ax2.set_xticklabels(component_names)\n",
    "ax2.set_ylabel('Parameters (Millions)')\n",
    "ax2.set_title('Parameter Count by Component')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(sizes):\n",
    "    ax2.text(i, v/1e6 + 1, f'{v/1e6:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nMost parameters are in the transformer blocks!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all model sizes and count parameters\n",
    "model_stats = {}\n",
    "for name, config in configs.items():\n",
    "    m = GPT2Model(config)\n",
    "    model_stats[name] = {\n",
    "        'layers': config.n_layer,\n",
    "        'embd': config.n_embd,\n",
    "        'heads': config.n_head,\n",
    "        'params': m.get_num_params() / 1e6,  # In millions\n",
    "    }\n",
    "\n",
    "# Print comparison table\n",
    "print('GPT-2 Model Size Comparison:')\n",
    "print('=' * 80)\n",
    "print(f'{\"Model\":15s} {\"Layers\":>8s} {\"Emb Dim\":>10s} {\"Heads\":>8s} {\"Parameters\":>15s}')\n",
    "print('=' * 80)\n",
    "for name, stats in model_stats.items():\n",
    "    print(f'{name:15s} {stats[\"layers\"]:>8d} {stats[\"embd\"]:>10d} {stats[\"heads\"]:>8d} {stats[\"params\"]:>12.1f}M')\n",
    "\n",
    "# Visualize scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "model_names = list(model_stats.keys())\n",
    "layers = [model_stats[n]['layers'] for n in model_names]\n",
    "embds = [model_stats[n]['embd'] for n in model_names]\n",
    "heads = [model_stats[n]['heads'] for n in model_names]\n",
    "params = [model_stats[n]['params'] for n in model_names]\n",
    "\n",
    "# Plot 1: Layers\n",
    "axes[0, 0].bar(range(len(model_names)), layers, color='steelblue')\n",
    "axes[0, 0].set_xticks(range(len(model_names)))\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Number of Layers')\n",
    "axes[0, 0].set_title('Model Depth')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Embedding dimension\n",
    "axes[0, 1].bar(range(len(model_names)), embds, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(model_names)))\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Embedding Dimension')\n",
    "axes[0, 1].set_title('Model Width')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Attention heads\n",
    "axes[1, 0].bar(range(len(model_names)), heads, color='mediumseagreen')\n",
    "axes[1, 0].set_xticks(range(len(model_names)))\n",
    "axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Number of Heads')\n",
    "axes[1, 0].set_title('Attention Parallelism')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Total parameters\n",
    "axes[1, 1].bar(range(len(model_names)), params, color='mediumpurple')\n",
    "axes[1, 1].set_xticks(range(len(model_names)))\n",
    "axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('Parameters (Millions)')\n",
    "axes[1, 1].set_title('Total Model Size')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(params):\n",
    "    axes[1, 1].text(i, v + 20, f'{v:.0f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nModel size scales with depth (layers), width (embd), and heads!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Weight Tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify weight tying between embeddings and LM head\n",
    "print('Weight Tying Verification:')\n",
    "print('=' * 60)\n",
    "\n",
    "# Check if they're the same object\n",
    "same_object = model.lm_head.weight is model.wte.weight\n",
    "print(f'lm_head.weight is wte.weight: {same_object}')\n",
    "\n",
    "# Check shapes\n",
    "print(f'\\nToken embeddings shape: {model.wte.weight.shape}')\n",
    "print(f'LM head weight shape: {model.lm_head.weight.shape}')\n",
    "\n",
    "# Calculate memory saved\n",
    "num_params = model.wte.weight.numel()\n",
    "param_size_mb = num_params * 4 / (1024 * 1024)  # 4 bytes per float32\n",
    "print(f'\\nMemory saved by weight tying: {param_size_mb:.2f} MB')\n",
    "print(f'Parameters saved: {num_params:,}')\n",
    "\n",
    "# Visualize the shared weights\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot token embedding weights\n",
    "wte_sample = model.wte.weight[:100, :50].detach().numpy()\n",
    "im1 = ax1.imshow(wte_sample, cmap='coolwarm', aspect='auto')\n",
    "ax1.set_xlabel('Embedding Dimension')\n",
    "ax1.set_ylabel('Token ID')\n",
    "ax1.set_title('Token Embeddings (wte.weight)\\nFirst 100 tokens, 50 dims')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Plot LM head weights (same as token embeddings)\n",
    "lm_head_sample = model.lm_head.weight[:100, :50].detach().numpy()\n",
    "im2 = ax2.imshow(lm_head_sample, cmap='coolwarm', aspect='auto')\n",
    "ax2.set_xlabel('Embedding Dimension')\n",
    "ax2.set_ylabel('Token ID')\n",
    "ax2.set_title('LM Head Weights (lm_head.weight)\\nSame as wte.weight!')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify they're truly identical\n",
    "print(f'\\nWeights are identical: {torch.equal(model.wte.weight, model.lm_head.weight)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Output Logits Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for a sample input\n",
    "model.eval()\n",
    "sample_input = torch.randint(0, small_config.vocab_size, (1, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(sample_input)\n",
    "\n",
    "# Get probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Analyze first position\n",
    "first_pos_logits = logits[0, 0].numpy()\n",
    "first_pos_probs = probs[0, 0].numpy()\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Logits distribution\n",
    "axes[0, 0].hist(first_pos_logits, bins=100, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Logit Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Logits Distribution (before softmax)')\n",
    "axes[0, 0].axvline(first_pos_logits.mean(), color='red', linestyle='--', label=f'Mean: {first_pos_logits.mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Probabilities distribution\n",
    "axes[0, 1].hist(first_pos_probs, bins=100, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Probability')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Probabilities Distribution (after softmax)')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Top-k probabilities\n",
    "top_k = 20\n",
    "top_probs, top_indices = torch.topk(probs[0, 0], top_k)\n",
    "axes[1, 0].bar(range(top_k), top_probs.numpy(), color='mediumseagreen')\n",
    "axes[1, 0].set_xlabel('Token Rank')\n",
    "axes[1, 0].set_ylabel('Probability')\n",
    "axes[1, 0].set_title(f'Top-{top_k} Token Probabilities')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative probability\n",
    "sorted_probs, _ = torch.sort(probs[0, 0], descending=True)\n",
    "cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "axes[1, 1].plot(cumsum[:1000].numpy(), color='mediumpurple', linewidth=2)\n",
    "axes[1, 1].axhline(0.9, color='red', linestyle='--', label='90% threshold')\n",
    "axes[1, 1].axhline(0.95, color='orange', linestyle='--', label='95% threshold')\n",
    "axes[1, 1].set_xlabel('Number of Tokens')\n",
    "axes[1, 1].set_ylabel('Cumulative Probability')\n",
    "axes[1, 1].set_title('Cumulative Probability Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find how many tokens needed for 90% and 95%\n",
    "tokens_90 = (cumsum >= 0.9).nonzero()[0].item() + 1\n",
    "tokens_95 = (cumsum >= 0.95).nonzero()[0].item() + 1\n",
    "print(f'Tokens needed for 90% probability: {tokens_90}')\n",
    "print(f'Tokens needed for 95% probability: {tokens_95}')\n",
    "print(f'Total vocabulary size: {small_config.vocab_size:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace Activations Through the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny model for visualization\n",
    "tiny_config = GPT2Config(n_embd=64, n_layer=4, n_head=4, n_positions=128)\n",
    "tiny_model = GPT2Model(tiny_config)\n",
    "tiny_model.eval()\n",
    "\n",
    "# Forward pass with hooks to capture intermediate activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "for i, block in enumerate(tiny_model.h):\n",
    "    block.register_forward_hook(get_activation(f'block_{i}'))\n",
    "\n",
    "tiny_model.ln_f.register_forward_hook(get_activation('final_ln'))\n",
    "\n",
    "# Forward pass\n",
    "sample = torch.randint(0, tiny_config.vocab_size, (1, 20))\n",
    "with torch.no_grad():\n",
    "    output = tiny_model(sample)\n",
    "\n",
    "# Visualize activations at each layer\n",
    "fig, axes = plt.subplots(1, len(tiny_model.h) + 1, figsize=(15, 3))\n",
    "\n",
    "for i in range(len(tiny_model.h)):\n",
    "    act = activations[f'block_{i}'][0].numpy()  # (seq_len, n_embd)\n",
    "    im = axes[i].imshow(act.T, cmap='coolwarm', aspect='auto')\n",
    "    axes[i].set_xlabel('Position')\n",
    "    axes[i].set_ylabel('Feature')\n",
    "    axes[i].set_title(f'Block {i}')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "# Final layer norm\n",
    "final_act = activations['final_ln'][0].numpy()\n",
    "im = axes[-1].imshow(final_act.T, cmap='coolwarm', aspect='auto')\n",
    "axes[-1].set_xlabel('Position')\n",
    "axes[-1].set_ylabel('Feature')\n",
    "axes[-1].set_title('Final LN')\n",
    "plt.colorbar(im, ax=axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Each layer transforms the representation!')\n",
    "print('Early layers learn basic patterns, later layers learn complex semantics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, config):\n",
    "    \"\"\"Print a comprehensive model summary.\"\"\"\n",
    "    print('GPT-2 Model Summary')\n",
    "    print('=' * 80)\n",
    "    print(f'Configuration:')\n",
    "    print(f'  Layers: {config.n_layer}')\n",
    "    print(f'  Embedding dimension: {config.n_embd}')\n",
    "    print(f'  Attention heads: {config.n_head}')\n",
    "    print(f'  Head dimension: {config.n_embd // config.n_head}')\n",
    "    print(f'  FFN inner dimension: {config.n_inner}')\n",
    "    print(f'  Vocabulary size: {config.vocab_size:,}')\n",
    "    print(f'  Context length: {config.n_positions}')\n",
    "    print(f'\\nDropout rates:')\n",
    "    print(f'  Embedding dropout: {config.embd_pdrop}')\n",
    "    print(f'  Attention dropout: {config.attn_pdrop}')\n",
    "    print(f'  Residual dropout: {config.resid_pdrop}')\n",
    "    print(f'\\nParameter count:')\n",
    "    breakdown = model.get_num_params_by_component()\n",
    "    for component, count in breakdown.items():\n",
    "        if component == 'total':\n",
    "            print('  ' + '-' * 40)\n",
    "        pct = (count / breakdown['total'] * 100) if breakdown['total'] > 0 else 0\n",
    "        print(f'  {component:25s}: {count:>12,} ({pct:>5.1f}%)')\n",
    "    print('=' * 80)\n",
    "\n",
    "# Print summary for GPT-2 small\n",
    "print_model_summary(model, small_config)\n",
    "\n",
    "print('\\n✅ Complete GPT-2 model assembled!')\n",
    "print('\\nNext step: Problem 11 - Weight Initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "test_config = GPT2Config(n_embd=256, n_layer=2, n_head=4)\n",
    "test_model = GPT2Model(test_config)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, test_config.vocab_size, (2, 10))\n",
    "test_output = test_model(test_input)\n",
    "\n",
    "print('✓ GPT2Model created successfully')\n",
    "print('✓ All components initialized')\n",
    "print('✓ Forward pass works')\n",
    "print('✓ Weight tying implemented')\n",
    "print(f'✓ Output shape correct: {test_output.shape}')\n",
    "print('\\n✅ Full GPT-2 model assembly complete!')\n",
    "print('\\nRun \"python -m pytest test_gpt2.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
