{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 11: Weight Initialization - Interactive Notebook\n",
    "\n",
    "Explore the importance of weight initialization and visualize how different initialization schemes affect model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../09-gpt2-config')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from solution import GPT2Model, GPT2Config\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with proper initialization\n",
    "config = GPT2Config(n_embd=256, n_layer=4, n_head=4)\n",
    "torch.manual_seed(42)\n",
    "model = GPT2Model(config)\n",
    "\n",
    "print('Model created with weight initialization')\n",
    "print(f'Total parameters: {model.get_num_params():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Weight Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect weights from different layer types\n",
    "linear_weights = []\n",
    "embedding_weights = []\n",
    "layernorm_weights = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        linear_weights.append(module.weight.detach().flatten())\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        embedding_weights.append(module.weight.detach().flatten())\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        layernorm_weights.append(module.weight.detach().flatten())\n",
    "\n",
    "# Concatenate all weights\n",
    "linear_weights = torch.cat(linear_weights).numpy()\n",
    "embedding_weights = torch.cat(embedding_weights).numpy()\n",
    "layernorm_weights = torch.cat(layernorm_weights).numpy()\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Linear weights\n",
    "axes[0, 0].hist(linear_weights, bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0, 0].axvline(linear_weights.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {linear_weights.mean():.6f}')\n",
    "axes[0, 0].axvline(0, color='green', linestyle='--', linewidth=1, alpha=0.5, label='Zero')\n",
    "axes[0, 0].set_xlabel('Weight Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title(f'Linear Layer Weights\\nStd: {linear_weights.std():.6f} (target: 0.02)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Embedding weights\n",
    "axes[0, 1].hist(embedding_weights, bins=100, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[0, 1].axvline(embedding_weights.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {embedding_weights.mean():.6f}')\n",
    "axes[0, 1].axvline(0, color='green', linestyle='--', linewidth=1, alpha=0.5, label='Zero')\n",
    "axes[0, 1].set_xlabel('Weight Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title(f'Embedding Weights\\nStd: {embedding_weights.std():.6f} (target: 0.02)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# LayerNorm weights\n",
    "axes[1, 0].hist(layernorm_weights, bins=50, alpha=0.7, color='mediumseagreen', edgecolor='black')\n",
    "axes[1, 0].axvline(layernorm_weights.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {layernorm_weights.mean():.6f}')\n",
    "axes[1, 0].axvline(1.0, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='Target: 1.0')\n",
    "axes[1, 0].set_xlabel('Weight Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'LayerNorm Weights (should be 1.0)\\nStd: {layernorm_weights.std():.6f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(linear_weights, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot: Linear Weights vs Normal Distribution')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Weight initialization statistics:')\n",
    "print(f'  Linear weights: mean={linear_weights.mean():.6f}, std={linear_weights.std():.6f}')\n",
    "print(f'  Embedding weights: mean={embedding_weights.mean():.6f}, std={embedding_weights.std():.6f}')\n",
    "print(f'  LayerNorm weights: mean={layernorm_weights.mean():.6f}, std={layernorm_weights.std():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Initialization Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models with different initializations for comparison\n",
    "def create_model_with_init(init_type, std=0.02):\n",
    "    \"\"\"Create a tiny model with specific initialization.\"\"\"\n",
    "    config = GPT2Config(n_embd=64, n_layer=2, n_head=4)\n",
    "    model = GPT2Model(config)\n",
    "    \n",
    "    if init_type == 'zero':\n",
    "        # All zeros (bad!)\n",
    "        for p in model.parameters():\n",
    "            if p.dim() >= 2:\n",
    "                nn.init.zeros_(p)\n",
    "    elif init_type == 'large':\n",
    "        # Too large std (bad!)\n",
    "        for p in model.parameters():\n",
    "            if p.dim() >= 2:\n",
    "                nn.init.normal_(p, 0, 1.0)\n",
    "    elif init_type == 'small':\n",
    "        # Too small std (bad!)\n",
    "        for p in model.parameters():\n",
    "            if p.dim() >= 2:\n",
    "                nn.init.normal_(p, 0, 0.0001)\n",
    "    # 'gpt2' keeps the proper initialization from __init__\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test different initializations\n",
    "init_types = ['zero', 'small', 'gpt2', 'large']\n",
    "results = {}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for init_type in init_types:\n",
    "    model = create_model_with_init(init_type)\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass\n",
    "    input_ids = torch.randint(0, config.vocab_size, (1, 10))\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "    \n",
    "    results[init_type] = {\n",
    "        'logits_mean': logits.abs().mean().item(),\n",
    "        'logits_std': logits.std().item(),\n",
    "        'logits_max': logits.abs().max().item(),\n",
    "    }\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['logits_mean', 'logits_std', 'logits_max']\n",
    "titles = ['Logits Mean (abs)', 'Logits Std', 'Logits Max (abs)']\n",
    "colors = {'zero': 'red', 'small': 'orange', 'gpt2': 'green', 'large': 'purple'}\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    values = [results[init_type][metric] for init_type in init_types]\n",
    "    bar_colors = [colors[init_type] for init_type in init_types]\n",
    "    \n",
    "    axes[idx].bar(range(len(init_types)), values, color=bar_colors, alpha=0.7)\n",
    "    axes[idx].set_xticks(range(len(init_types)))\n",
    "    axes[idx].set_xticklabels(init_types)\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].set_title(title)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(values):\n",
    "        axes[idx].text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Initialization comparison results:')\n",
    "print('=' * 70)\n",
    "for init_type in init_types:\n",
    "    print(f'{init_type:10s}: {results[init_type]}')\n",
    "print('\\nGPT-2 initialization (green) produces the most reasonable outputs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Layer-by-Layer Weight Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weights layer by layer\n",
    "layer_stats = []\n",
    "\n",
    "for i, block in enumerate(model.h):\n",
    "    # Attention weights\n",
    "    attn_mean = block.attn.c_attn.weight.mean().item()\n",
    "    attn_std = block.attn.c_attn.weight.std().item()\n",
    "    \n",
    "    # FFN weights\n",
    "    ffn_mean = block.mlp.c_fc.weight.mean().item()\n",
    "    ffn_std = block.mlp.c_fc.weight.std().item()\n",
    "    \n",
    "    layer_stats.append({\n",
    "        'layer': i,\n",
    "        'attn_mean': attn_mean,\n",
    "        'attn_std': attn_std,\n",
    "        'ffn_mean': ffn_mean,\n",
    "        'ffn_std': ffn_std,\n",
    "    })\n",
    "\n",
    "# Plot layer-by-layer statistics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "layers = [s['layer'] for s in layer_stats]\n",
    "attn_means = [abs(s['attn_mean']) for s in layer_stats]\n",
    "attn_stds = [s['attn_std'] for s in layer_stats]\n",
    "ffn_means = [abs(s['ffn_mean']) for s in layer_stats]\n",
    "ffn_stds = [s['ffn_std'] for s in layer_stats]\n",
    "\n",
    "# Mean plot\n",
    "ax1.plot(layers, attn_means, 'o-', label='Attention', linewidth=2, markersize=8, color='steelblue')\n",
    "ax1.plot(layers, ffn_means, 's-', label='FFN', linewidth=2, markersize=8, color='coral')\n",
    "ax1.axhline(0, color='green', linestyle='--', alpha=0.5, label='Target: 0')\n",
    "ax1.set_xlabel('Layer Index')\n",
    "ax1.set_ylabel('Absolute Mean')\n",
    "ax1.set_title('Weight Mean by Layer')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Std plot\n",
    "ax2.plot(layers, attn_stds, 'o-', label='Attention', linewidth=2, markersize=8, color='steelblue')\n",
    "ax2.plot(layers, ffn_stds, 's-', label='FFN', linewidth=2, markersize=8, color='coral')\n",
    "ax2.axhline(0.02, color='green', linestyle='--', alpha=0.5, label='Target: 0.02')\n",
    "ax2.set_xlabel('Layer Index')\n",
    "ax2.set_ylabel('Standard Deviation')\n",
    "ax2.set_title('Weight Std by Layer')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('All layers initialized consistently!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test activation magnitudes through the network\n",
    "model.eval()\n",
    "\n",
    "# Create hooks to capture activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "for i, block in enumerate(model.h):\n",
    "    block.register_forward_hook(get_activation(f'block_{i}'))\n",
    "\n",
    "# Forward pass\n",
    "input_ids = torch.randint(0, config.vocab_size, (1, 20))\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "\n",
    "# Analyze activation statistics\n",
    "activation_stats = []\n",
    "for i in range(len(model.h)):\n",
    "    act = activations[f'block_{i}']\n",
    "    activation_stats.append({\n",
    "        'layer': i,\n",
    "        'mean': act.abs().mean().item(),\n",
    "        'std': act.std().item(),\n",
    "        'max': act.abs().max().item(),\n",
    "    })\n",
    "\n",
    "# Plot activation statistics\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "layers = [s['layer'] for s in activation_stats]\n",
    "means = [s['mean'] for s in activation_stats]\n",
    "stds = [s['std'] for s in activation_stats]\n",
    "maxs = [s['max'] for s in activation_stats]\n",
    "\n",
    "ax1.plot(layers, means, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "ax1.set_xlabel('Layer Index')\n",
    "ax1.set_ylabel('Mean Absolute Activation')\n",
    "ax1.set_title('Activation Mean by Layer')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(layers, stds, 'o-', linewidth=2, markersize=8, color='coral')\n",
    "ax2.set_xlabel('Layer Index')\n",
    "ax2.set_ylabel('Activation Std')\n",
    "ax2.set_title('Activation Std by Layer')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "ax3.plot(layers, maxs, 'o-', linewidth=2, markersize=8, color='mediumseagreen')\n",
    "ax3.set_xlabel('Layer Index')\n",
    "ax3.set_ylabel('Max Absolute Activation')\n",
    "ax3.set_title('Activation Max by Layer')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Activation magnitudes are stable across layers!')\n",
    "print('This is a sign of good initialization.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient flow through the network\n",
    "model.train()\n",
    "\n",
    "# Forward and backward pass\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 10))\n",
    "logits = model(input_ids)\n",
    "loss = logits.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Collect gradient statistics by layer\n",
    "gradient_stats = []\n",
    "for i, block in enumerate(model.h):\n",
    "    attn_grad = block.attn.c_attn.weight.grad\n",
    "    ffn_grad = block.mlp.c_fc.weight.grad\n",
    "    \n",
    "    gradient_stats.append({\n",
    "        'layer': i,\n",
    "        'attn_grad_mean': attn_grad.abs().mean().item(),\n",
    "        'attn_grad_max': attn_grad.abs().max().item(),\n",
    "        'ffn_grad_mean': ffn_grad.abs().mean().item(),\n",
    "        'ffn_grad_max': ffn_grad.abs().max().item(),\n",
    "    })\n",
    "\n",
    "# Plot gradient flow\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "layers = [s['layer'] for s in gradient_stats]\n",
    "attn_grads = [s['attn_grad_mean'] for s in gradient_stats]\n",
    "ffn_grads = [s['ffn_grad_mean'] for s in gradient_stats]\n",
    "attn_max = [s['attn_grad_max'] for s in gradient_stats]\n",
    "ffn_max = [s['ffn_grad_max'] for s in gradient_stats]\n",
    "\n",
    "# Mean gradients\n",
    "ax1.plot(layers, attn_grads, 'o-', label='Attention', linewidth=2, markersize=8, color='steelblue')\n",
    "ax1.plot(layers, ffn_grads, 's-', label='FFN', linewidth=2, markersize=8, color='coral')\n",
    "ax1.set_xlabel('Layer Index')\n",
    "ax1.set_ylabel('Mean Absolute Gradient')\n",
    "ax1.set_title('Gradient Flow: Mean by Layer')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Max gradients\n",
    "ax2.plot(layers, attn_max, 'o-', label='Attention', linewidth=2, markersize=8, color='steelblue')\n",
    "ax2.plot(layers, ffn_max, 's-', label='FFN', linewidth=2, markersize=8, color='coral')\n",
    "ax2.set_xlabel('Layer Index')\n",
    "ax2.set_ylabel('Max Absolute Gradient')\n",
    "ax2.set_title('Gradient Flow: Max by Layer')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Gradients flow through all layers without vanishing or exploding!')\n",
    "print('This enables effective learning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "test_config = GPT2Config(n_embd=256, n_layer=2, n_head=4)\n",
    "torch.manual_seed(42)\n",
    "test_model = GPT2Model(test_config)\n",
    "\n",
    "# Check initialization\n",
    "linear_layer = test_model.h[0].attn.c_attn\n",
    "mean = linear_layer.weight.mean().item()\n",
    "std = linear_layer.weight.std().item()\n",
    "\n",
    "print('✓ Model created with weight initialization')\n",
    "print(f'✓ Linear weights: mean={mean:.6f}, std={std:.6f}')\n",
    "print(f'✓ Target: mean≈0, std≈0.02')\n",
    "\n",
    "# Check LayerNorm\n",
    "ln = test_model.h[0].ln_1\n",
    "ln_all_ones = torch.all(ln.weight == 1.0).item()\n",
    "ln_all_zeros = torch.all(ln.bias == 0.0).item()\n",
    "\n",
    "print(f'✓ LayerNorm weights all 1.0: {ln_all_ones}')\n",
    "print(f'✓ LayerNorm biases all 0.0: {ln_all_zeros}')\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, test_config.vocab_size, (2, 10))\n",
    "test_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = test_model(test_input)\n",
    "\n",
    "has_nan = torch.isnan(test_output).any().item()\n",
    "has_inf = torch.isinf(test_output).any().item()\n",
    "\n",
    "print(f'✓ Forward pass works: output shape {test_output.shape}')\n",
    "print(f'✓ No NaN values: {not has_nan}')\n",
    "print(f'✓ No inf values: {not has_inf}')\n",
    "\n",
    "print('\\n✅ Weight initialization complete!')\n",
    "print('\\nRun \"python -m pytest test_initialization.py -v\" for complete validation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
